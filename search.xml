<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flink - 有状态的实时计算]]></title>
    <url>%2F2020%2F05%2F06%2FFlink%20-%20%E6%9C%89%E7%8A%B6%E6%80%81%E7%9A%84%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[本文主要从使用层面介绍Flink独树一帜的设计, 帮助大家理解并更好的利用这些特性 Apache Flink的定义Apache Flink是在无边界和有边界的数据流上进行有状态计算的框架. 同行对比面对已经在实时领域耕耘了许久的两位老前辈: Storm和Spark Streaming, Flink有什么优势能够脱颖而出呢? Spark Streaming 攒一段数据再计算, 本质还是批处理, 涉及到shuffle还是会有落盘, fetch和merge等操作. API用起来很流畅, 但是不适合维护State. Spark Streaming更适合做etl. Storm Flink API跟Spark Streaming很像，但编程模型跟Storm更为接近。Storm自身不支持State，需要用户自己来维护，为了高可用和故障恢复, 用户通常会选择Redis做缓存，不过也由此也增加了对外部系统的依赖，同时也带来了额外的开销。 由此可见, Storm和Spark Streaming在State的处理上都有些力不从心, Flink与之相比最大的优势就是提供了完善的State的支持. State每个重要的流应用程序都是有状态的, 只有少数仅对事件做转换, 并且事件彼此独立的应用程序不需要状态. 任何运行基本业务逻辑的程序都需要记住事件或中间结果, 以便在后续的处理中访问它们. 从Flink在state处理上下文中提供的所有特性可以看到, state是Flink里的一等公民. 多个state原型 flink为不同的数据结构提供了state原型, 比如原子value、list或map. 开发者可以结合业务逻辑选择合适的state原型. 可插拔的state Backend 不同的后端state存放的位置不一样, 比如你可以配置RocksDBStateBackend, flink会把state保存在RocksDB中. RocksDB是一个高效的嵌入式磁盘KV数据库, 类似于HBASE Exactly-once一致性状态保证 flink的检查点与故障恢复机制保证了应用程序的state在崩溃时的一致性, 因此失败是透明的, 不会影响应用的准确性. 支持超大state flink基于异步和增量的算法, 可以支持TB级别的state 应用可拓展 flink为应用程序的拓展提供了强大的兼容性支持, 不论业务逻辑修改还是并行度变更, 你可以从容的应对业务的迭代 关键特性event-time与watermarkflink支持3种时间模型: Processing time 指数据进入到操作符的系统时间. 是最简单的时间概念, 不需要在流与机器之间协调. 它可以提供最好的性能和最低的延迟. 由于processing time是运行时指定的, 因此程序在这种时间模型下, 每次执行的结果都不一样. Event time 是记录实际产生的时间. 在进入flink之前就已经内嵌在记录当中. 在event-time, 程序处理的进度有数据来决定, 跟机器时钟没有关系. event-time程序必须定义如何产生水位线watermark, 水位线可以描述程序的进度. event-time可以反映客观事实, 基于事件时间的程序的计算结果是最准确的, 并且每次执行结果都不变. Ingestion time 数据进入flink的时间, source操作符用其当前时间作为每个记录的时间, 基于事件的操作符(比如window)都会引用这个时间. watermarkwatermark是flink用来评估程序进度的机制. 在实际环境中数据不可避免的会产生乱序, watermark就是用来告诉flink低于它的记录都已经处理完毕了. watermark作为stream的一部分随着记录在stream中流动, 它的本质就是一个时间戳. 在乱序的流中, watermark意味着早于它的记录都已经到达flink. 如下图 有的操作符消费多条输入流, 例如union, join, 或者跟在keyBy()或partition()函数后面的操作符, 这些操作符的当前事件时间就是它的输入流中最小的事件时间. 如何理解watermark呢 假设一个场景, flink收到的数据绝大多数都是顺序的, 偶尔有几个乱序, 但最多不会超过1min. 即00:00产生的记录最迟01:00可以被flink消费到. 反过来讲, 当前时刻01:00, flink可以拿到最早00:00产生的记录; 02:00拿到最早01:00产生的记录…以此类推. 那么相对于当前时间T, 水位线就是T-1. 即当前时间减去1分钟, 在这之前的记录都已处理完毕, 不会再有更早的记录出现. 这是建立在消费紧跟着当前时间来模拟的. 对应到event-time, “当前时间”就可以认为是当前接收到记录的最大事件时间, 上面的公式就变成: max(T) - 1 如何生成watermark呢 接上述例子, 数据有1分钟的乱序 123456789env.addSource(...) .flatMap(new EventExtraction()) .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.minutes(1)) &#123; @Override public long extractTimestamp(Event element) &#123; return element.getEventTime(); &#125; &#125;); window窗口是处理无界流的心脏. 窗口将stream切分成了有宽度的”Buckets”, 在这之上我们可以进行计算. 123456789stream .keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: "assigner" [.trigger(...)] &lt;- optional: "trigger" (else default trigger) [.evictor(...)] &lt;- optional: "evictor" (else no evictor) [.allowedLateness(...)] &lt;- optional: "lateness" (else zero) [.sideOutputLateData(...)] &lt;- optional: "output tag" (else no side output for late data) .reduce/aggregate/fold/apply() &lt;- required: "function" [.getSideOutput(...)] &lt;- optional: "output tag" 篇幅原因, 我们主要讨论keyed window, 以及其中keyBy, window, trigger和function api window的生命周期 以[12:00, 12:05)的窗口为例, 属于这个窗口的第一个元素到达window操作符时, 这个窗口被创建. 当水位线越过12:06(水位线始终比当前最大事件时间小1分钟)时, 窗口被完全删除 window function flink有两类window function 可累加的函数 reduce, aggregate, fold就属于这类函数. 它们每收到一条记录就进行计算, 记录不在内存中停留, 只会保留聚合结果. 这种函数由于是增量计算, 且不会存储大量数据, 所以效率很高. 但是能够处理的场景有限, 比如uv没法处理, 另外这类函数拿不到窗口的上下文来访问时间与state信息. 12345678910DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123; public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) &#123; return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1); &#125; &#125;); ProcessWindowFunction 可以拿到包含了窗口所有记录的Iterable对象, 和一个包含了Time与State信息的Context对象, 这使得它提供了比其它函数更具灵活性. 不过这是以牺牲了性能与资源消耗为代价, 因为它无法增量计算, window的元素都缓存在内存中. 123456789101112131415161718192021DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(t -&gt; t.f0) .timeWindow(Time.minutes(5)) .process(new MyProcessWindowFunction());/* ... */public class MyProcessWindowFunction extends ProcessWindowFunction&lt;Tuple2&lt;String, Long&gt;, String, String, TimeWindow&gt; &#123; @Override public void process(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) &#123; long count = 0; for (Tuple2&lt;String, Long&gt; in: input) &#123; count++; &#125; out.collect("Window: " + context.window() + "count: " + count); &#125;&#125; 触发器 触发器决定了窗口函数何时执行, flink用4个常量来描述触发时的行为： CONTINUE: 什么都不做 FIRE: 触发计算 PURGE: 清空窗口里的元素 FIRE_AND_PURGE: 触发计算并在之后清空窗口里的元素 默认情况下窗口会在watermark越过窗口的结束时间时调用。比如5min宽度的窗口[12:00,12:05)，1min乱序的watermark，会在12:06结束时触发计算。 基于processing-time的程序由于没有watermark，所以会在12:05结束时触发 用户可能不愿意等到6min之后才看到结果，最好是每30s计算一个结果。对于event-time的程序，flink在不破坏watermark机制的前提下用Pane巧妙的解决了这个问题。 基于processing-time的程序无法做到在窗口期内触发计算 Pane是flink根据触发间隔（本例中是30s）而在逻辑上划分的数据块（block），即一个window会划分成n个pane，n ＝ window_size / pane_size。当水位线越过pane的end_tine时触发计算，计算的数据集就是pane里的数据。 注意 如果窗口函数是ProccessFunction，则窗口里的数据默认会在窗口销毁时清空。换句话说pane会包含之前pane的数据，这部分数据如何处理呢？flink提供了触发器的包装类PurgingTrigger，它会在每次pane触发计算后清空窗口的数据。 1234567source .keyBy(Order::getCategoryCode) .window(TumblingEventTimeWindows.of(Time.hours(1))) .trigger(PurgingTrigger.of(ContinuousEventTimeTrigger.of(Time.seconds(30)))) .process(new TotalMetric(startFrom, false)) .name("transform-alluser-total") .uid("transform-alluser-total"); checkpointFlink提供了一种容错机制，可以恢复应用程序的state到最近的一致性状态。该机制可确保即使出现故障，程序的状态最终也将以Exactly-Once的语义回溯数据流中的每条记录。这个机制就是Checkpoint. 当应用程序失败时, flink会停止数据流, 系统会重启程序并将操作符重置到最近一次成功的checkpoint. 重启的程序处理的所有记录都会保证不与前一次checkpoint里的数据重复. flink分布式快照的核心是stream栅栏(Barriers). 这些Barrier从source开始注入到数据流中, 作为流的一部分随着记录一起发送到下游. 如图barriers将数据流分割成了若干部分, 每个barrier前面的记录进入到当前的快照, 之后的记录会进入下一次快照. Barriers不会打断数据流因此它非常轻量. 不同快照的barriers可以同时出现在数据流中, 这意味着多个快照可能会并发执行. Barrier从source开始注入到流中, 当中间的操作符收到上游发送过来的全部barrier, 它在完成state快照后会把barrier发送给它所有的下游stream. 一旦sink收到了它所有上游stream的barrier, 会向checkpoint协调器发送快照完毕的ack消息. 所有的sink发送ack后快照就认为执行成功了. 快照的成功标志着属于当前快照的所有记录(包含产生的所有子孙记录)都已经通过了完整的数据流拓扑. 一旦操作符接收到了上游某一个stream的barrier, 则在收到其它stream的barrier之前都不会再处理这个stream的数据. 否则它会把分属于两个快照的数据混合到一起 在等待其它stream的barrier的同时, 操作符会将来自这个stream的数据缓存下来 当收到最后一个barrier, 操作符在完成自身的输出后把自己的barrier也发送出去 最后, 它开始恢复处理记录, 先处理缓存中的, 然后是来自stream的 注意 align buffer是flinkExactly-Once语义的保证, 只有在当前快照的记录都处理完了才会处理下一份快照的数据. 如果启用的是At-Least-Once语义, 则操作符会跳过align, 持续处理记录. 这样记录可能会在多份快照中重复. flink默认开启Exactly-Once, 手动开启的方法 1CheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); statebackendflink提供了3个开箱即用的backend: MemoryStateBackend 将数据以Objects的形式存放到Java Heap, Key/Value state和window操作符用Hash表来存放values, 触发器等等. checkpoint触发时, 会将state的快照作为ack的一部分发送到JobManager, JobManager将它们存放到自己的堆内存中. 适合在本地开发和调试使用. FsStateBackend 需要配置一个文件系统的url, 比如”hdfs://namenode:8020/flink/checkpoints”或”file:///data/flink/checkpoints”. FsStateBackend同样是将state存放到内存中, checkpoint发生时将快照写到配置的文件系统中. 由于state存放在内存, 对其的访问都非常高效, 序列化只会在checkpoint时发生. 这个Backend的大小受限于JVM, 有OutOfMemorys的风险. RocksDBStateBackend 也需要配置一个文件系统url, RocksDBStateBackend将state存放到RocksDB数据库中, RocksDB把数据文件存放到TaskManager的本地目录中. checkpoint触发时, RocksDBStateBackend将快照存放到配置的文件系统中. 这个Backend支持的state大小受限于可用的磁盘空间. RocksDBStateBackend是唯一一个支持增量checkpoint的Backend 与FsStateBackend不同, RocksDBStateBackend每次访问state都要经过序列化/反序列化, 因此会有一定的性能损耗. 如何选择合适的backend呢 如果数据量不大, 足以放入内存, 那么使用FsStateBackend可以获得更高的性能. 否则的话推荐使用RocksDBStateBackend 这个数据量的边界可以参考现有实时看板的DAU, 处理最高150w的用户id绰绰有余. State Time-To-Live (TTL)flink提供了ttl机制, 所有的集合类型的state都支持记录级别的ttl, 这意味着list元素或map记录可以单独过期. ttl在RocksDBStateBackend时非常有用. 因为手动删除(state.clear())只是标记删除, state的体积不会减少, 只有在下次访问时才会真正删除. 所以RocksDBStateBackend的state会越来越大, 对于长期驻留在state里的顽固记录只有在下次启动程序时删除(因为RocksDB只会加载访问的数据). 这在其它两个backend上不会发生 ttl的配置非常简单 12345678910111213import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.days(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .cleanupInRocksdbCompactFilter(1000) .build();ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;("text state", String.class);stateDescriptor.enableTimeToLive(ttlConfig); 注意 flink目前(1.10版本)只支持ProcessingTime的过期, EventTime的ttl会在后续的版本里添加. uid任何程序不可避免的会产生迭代, 当业务逻辑改变时, flink如何让旧的state兼容新的程序呢? 这里要分两个部分来讨论 应用程序的拓扑改变了 当应用程序从检查点重启时, flink将存储在保存点中的state与应用程序的有状态运算符进行匹配, 这个匹配就是基于操作符的id来实现的. 每个操作符都有一个默认ID，该ID根据操作符在程序拓扑中的位置来决定. 因此没有修改的程序总是可以从检查点里恢复. 然而默认的id可能会随着程序的改变而改变, 开发者因此为操作符需要明确分配一个id, 只要id不变程序就能正常重启. 分配id很简单: 1val mappedEvents: DataStream[(Int, Long)] = events.map(new MyStatefulMapFunc()).uid("mapper-1") 由于operator ID存放在检查点中, 必须跟程序启动时的id相等, 因此建议给所有将来有可能会升级的操作符分配一个唯一的ID 程序的并行度发生改变 对于Operator state, state不会固定属于某一个子任务, flink在程序重启时采用轮询的方式为子任务分配state 对于Keyed state, 每个Keyed state逻辑上绑定到一个唯一的&lt;parallel-operator-instance, key&gt;组合, 一个key只会属于一个唯一的并行实例. 这个组合又被组织成Key Groups, key group由若干连续的key组成, 是flink重新分发state的原子单位, key group的数量等于程序最大并行度. 当程序的并行度发生改变, 每个操作符的并行实例分配到的key group随之变化, 但key相对于最高并行度的映射关系没有变, 因此可以任意拓展, 这其实就是一致性hash的实现.]]></content>
      <tags>
        <tag>flink 实时计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用MySQL实现leader选举]]></title>
    <url>%2F2019%2F12%2F12%2F%E7%94%A8MySQL%E5%AE%9E%E7%8E%B0leader%E9%80%89%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[前言为了保证服务的高可用, 我们通常会将其部署到多个节点(或者多个容器). 但是某些异步任务我们希望只交给一个实例来处理, 或者说, 要在这些服务中选出一个leader. zookeeper的菜谱里有一道招牌菜就是leader election, 不过今天我们不打算点这道菜, 看看用随手可得的MySQL能否满足我们的需求. leader-election关于leader选举我们知道: 选举是一个低频的动作, 只是在竞选的时候有一定的并发 选举完成leader和follower都要保持会话(比如心跳), leader挂掉所有的follower可以及时参加选举 如果leader从假死状态中恢复发现自己已经不是leader了, 则自动变身为follower 如果leader和follower之间没有交互, 各服务实例只需要知道自己是不是leader就行了 在这里MySQL扮演的是一个第三方服务, 来协调leader和follower之间的关系, 不过MySQL本身是没有这个功能的, 我们可以利用它的一些特性来实现. 预备知识选举时, 多个实例只有一个能竞争成功, 用CAS算法再合适不过了. MySQL的CAS可以用版本号来实现. 比如有一个表如下字段: id, key, value, version(int), 每次更新value时都带着版本号 1update tbl set value = ?, version = version + 1 where key = ? version = ? 更新前先获得当前的版本号, 比如初始版本是1, 则大家都拿到了1 所有人都在执行上述更新, 都想把version变成2, MySQL保证了同一时刻只会有一个人实现了他的愿望 唯一的幸运儿更新完成后, 其他人拿着1就变成了脏数据, 自然更新失败 至此leader已经选出来了, 没有加锁也无需自旋, 是不是很简单呢! 设计初次选举过程 初始版本号为0, 服务实例初始化时都试图更新选举版本号, 但只有一个成功, 成功者成为leader leader定时向MySQL发送心跳, 维护来之不易的版本号 follower定时探测leader最后一次心跳时间, 如果超过时限(比如5s)就认为leader宕机 leader宕机之后的改选 follower探测到leader心跳中断, 不论是卡主还是宕机, 都认为leader挂掉 获取最后一次心跳的版本号, 重复上述选举过程 实现竞争表12345678910CREATE TABLE `leader_election` ( `elect_key` varchar(255) NOT NULL COMMENT '选举的key', `version` bigint NOT NULL DEFAULT 0 COMMENT '当前版本号, 每次选举和心跳都会在此字段进行cas', `tick_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '心跳时间', `leader_id` bigint NOT NULL DEFAULT 0 COMMENT 'leader所在实例的id', `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`elect_key`), KEY `idx_version` (`version`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='leader选举'; 这里不需要value字段 leader会定时发送tick_time, follower也会定时探测leader的心跳时间, 超过一定时间就认为leader已经宕机了, 开始重新选举 leader_id可有可无, 如果leader于follower之间有交互这里可以换成ip:port 服务实现选举服务具备以下职责 tick 是leader的操作, 向MySQL发送心跳, 就是将version自增然后用CAS写入 doesLeaderCrashed follower的操作, 探测leader是否宕机, 不管leader是刚好卡主还是真的宕掉了, 只要最后一次心跳距今超过5s就算宕机 elect 初次选举, 初始版本号固定为0 reelect 改选, leader宕机之后的再次选举, 初始版本设定为最后一次心跳的版本 isLeader 供其他服务调用, 判断自身实例是否是leader getServiceId 每个服务都有一个全局唯一的id leader的心跳和follower的探活都由定时任务来发起1234567891011121314151617181920212223242526272829303132333435363738394041@Autowiredprivate ElectionService electionService;/** leader: 发送心跳 */@Async@Scheduled(cron = "*/5 * * * * ?")public void tick() &#123; if (!electionService.isLeader()) &#123; return; &#125; if (electionService.tick()) &#123; log.info("leader: 心跳发送成功, 继续作威作福"); &#125; else &#123; log.info("leader: 心跳发送失败, 失去leader资格"); &#125;&#125;/** follower: 虎视眈眈, 持续探测leader是否宕机, 如果宕机则重新开始选举 */@Async@Scheduled(cron = "*/5 * * * * ?")public void detectAndReelect() &#123; try &#123; // 这里停留1s是为了当主宕机了, follower可以尽快发现 // 如果不停留则有可能等到下一波探活才能发现 Thread.sleep(1000); &#125; catch (InterruptedException ignored) &#123; &#125; if (electionService.isLeader()) &#123; return; &#125; if (electionService.doesLeaderCrashed()) &#123; if (electionService.reelect()) &#123; log.info("follower探测leader状态: eader宕机, 通过竞选, 成功上位"); &#125; else &#123; log.info("follower探测leader状态: eader宕机, 竞选失败, 再等机会"); &#125; &#125; else &#123; log.info("follower探测leader状态: leader在岗, 等待机会"); &#125;&#125; 选举服务的具体实现初始化变量123456789101112131415private String key = "leader-election"; // 选举的keyprivate long tickIntervalMillis = 5000; // 心跳间隔, 毫秒private AtomicLong version = new AtomicLong(0L); // 如果竞选leader成功, 则要维护版本号private AtomicBoolean isLeader = new AtomicBoolean(false); // 当前是否是leader// 初始化选举是必须的@PostConstructprivate void init() &#123; boolean won = elect(); if (won) &#123; log.info("初选成功, 成为leader: version-&gt; &#123;&#125;, id-&gt; &#123;&#125;", version, id); &#125; else &#123; log.info("初选失败, 不骄不躁, 勤勤恳恳, 努力工作"); &#125;&#125; 选举的过程12345678910111213141516171819202122232425262728293031323334@Transactional@Overridepublic boolean elect() &#123; Election e = electionMapper.selectOne(key); if (e == null) &#123; throw new ElectionException("没有找到elect_key: " + key); &#125; return electing(0);&#125;@Transactional@Overridepublic boolean reelect() &#123; Election e = electionMapper.selectOne(key); if (e == null) &#123; throw new ElectionException("没有找到elect_key: " + key); &#125; return electing(e.getVersion());&#125;private boolean electing(long initVersion) &#123; boolean updated = compareAndSetVersionAndLeaderId(initVersion); if (updated) &#123; log.debug("竞选leader成功, 版本号置为1"); version.set(1); &#125; else &#123; log.debug("竞选leader失败, 等下次吧"); &#125; isLeader.set(updated); return updated;&#125; 心跳123456789101112131415@Transactional@Overridepublic boolean tick() throws ElectionException &#123; if (!isLeader()) &#123; throw new ElectionException("当前不是leader, 不能发送心跳"); &#125; boolean updated = compareAndSetVersion(version.get(), version.incrementAndGet()); isLeader.set(updated); if (!updated) &#123; version.set(0L); &#125; return updated;&#125; 探活follower的动作, 检查leader上次探活距今有没有超过5s, 超过就认为leader宕掉了 1234567891011121314151617181920public boolean doesLeaderCrashed() throws ElectionException &#123; if (isLeader()) &#123; throw new ElectionException("当前不是follower, 不能探测leader是否活跃"); &#125; Election e = electionMapper.selectOne(key); if (e == null) &#123; throw new ElectionException("没有找到elect_key: " + key); &#125; long tickTime = e.getTickTime().getTime(); long currTime = System.currentTimeMillis(); if ((currTime - tickTime) &gt; tickIntervalMillis) &#123; log.debug("leader嗝屁了, 终于熬出头了"); return true; &#125; else &#123; log.debug("leader仍然在岗, 别灰心, 持续探测"); return false; &#125;&#125; 总结用MySQL实现的leader选举, 没有引入其它组件, 全程没有加锁, 没有过多对MySQL的操作, 实现简单高效. 不过仍然有以下不足: leader的保持全靠自己的心跳, 时效性差, 一旦假死则有可能下次心跳才能发现自己挂了 重新选举没法立即进行, 仍然依赖自身的探活机制 无法避免羊群效应 综上所述, 如果你的服务对选举的及时性有强制要求, 而且引入其它组件(如zookeeper)又不太方便的时候, 不妨尝试一下这个方法, 不会让你失望哦! (刘蒙, liumeng.trm@foxmail.com) 参考资料 面试必备之乐观锁与悲观锁]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>leader-election</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】列式数据库]]></title>
    <url>%2F2019%2F09%2F29%2F%E3%80%90%E8%AF%91%E3%80%91%E5%88%97%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[翻译自Column-oriented DBMS Column-oriented DBMS面向列的DBMS（或列式数据库管理系统）是按列而不是按行存储数据表的数据库管理系统。在关系型数据库管理系统领域，使用列式存储和行式存储并没有什么区别。列和行数据库都可以使用传统的数据库查询语言（如sql）来加载数据并执行查询。列和行数据库都可以成为系统的支柱，为通用extract-transform-load（ETL）和数据可视化系统提供数据。然而，将数据存储在列中而不是行中，数据库可以更精确的访问到某个查询请求需要的数据，而不是扫描并丢弃行中不需要的数据。查询性能由此而提高，尤其是在非常大的数据集中。 描述背景关系型数据库管理系统的数据描述了一个包含列和行的二维表。比如说一个数据库里有这样一张表： RowId EmpId Lastname Firstname Salary 001 10 Smith Joe 40000 002 12 Jones Mary 50000 003 11 Johnson Cathy 44000 004 22 Jones Bob 55000 这个简单的表包含了一个员工的标识（EmpId）、名称字段（Lastname和Firstname）和工资。这个二维表的格式是一种抽象，实际的实现中硬件存储会要求数据被序列化成某一种格式。 涉及到硬盘的最昂贵的操作是寻址，为了改善整体的性能，应该用可以最小化寻址次数的方式存储关联数据。这就是人们熟知的Locality of reference，基本概念出现在不同的上下文环境中。硬盘以一些列固定大小的块组织起来，通常可以存放表里若干行数据。通过组织表的数据，使行适应这些块，并将相邻的行分组到连续的块中。在很多情况下，需要读取或寻找的块数和寻址的次数一起被最小化了（译者注：这句没懂…）。 面向行的系统存储一张表的通用做法是序列化每一行数据，就像这样： 1234001:10,Smith,Joe,40000;002:12,Jones,Mary,50000;003:11,Johnson,Cathy,44000;004:22,Jones,Bob,55000; 当数据插入表中时，它被分配一个内部的id，这个rowid字段在系统内部被用来关联数据。在这种情况下，记录具有独立于用户指定的empid的顺序rowid。这个例子当中，DBMS用短整型（short integer）存放rowid，实际使用会用更大的数字，比如64位、128位。 基于行的系统被设计的用尽可能少的操作，就可以有效的返回整行或记录数据。这适合一些常见的用例，比如系统尝试获取某一特定对象的信息，像是名片系统里用户的联系信息，或是线上购物系统里的商品信息。通过将这些数据和其它相邻数据存放到一个单独的块中，系统可以很快的用最少的操作获取到记录。 与一小部分特定的数据相反，行式系统在整张表上做集合范围（set-wide）的操作往往很低效。例如在这个例子里，要想找到工资在40000到50000之间的全部记录，DBMS必须完全遍历整个表来寻找匹配的记录。在上述的表格里数据可能在一个单独的块中，有上百行记录的表就不会这样，这时候就需要用多次磁盘操作来获取数据，并检查它。 为了提高这些操作的性能（这是很常见的，并且也是DBMS的关键点），大多数DBMS支持使用索引，将一列里所有的数据和可以指引到原始表的rowid存放到一起。salary列的索引看起来是这个样子的： 1234001:40000;003:44000;002:50000;004:55000; 由于它们只存储了一小片数据，而不是整行，索引总的来说要比主表要小。扫描这个小数据集减少了磁盘的操作。如果索引被重度使用，它可以显著的减少常见操作的时间。然而维护索引增加了系统的开销，尤其是新数据写入到数据库的时候，记录不仅要存放到主表，任何附加的索引都要同步更新。 在某一列或某些列的数据库索引通常都是按列存储，因此范围查找的操作（比如上面说的“获取工资在40000到50000的全部记录”）会非常快。 一些行数据库被设计的完全填充到内存中，成为一个内存数据库，这些系统不依赖磁盘操作，而且对于整个数据集有相等的访问时间。这样就不需要索引了，因为在典型的聚合场景中，扫描原始数据和整个索引所需要的操作数量是相同的。这样的系统可能更简单更小，但是只能管理能放进内存的数据库。 面向列的系统面向列的数据库将一列的全部数据序列化到一起，然后是另外一列，以此类推。例如我们的样例表，数据存储的风格是这样的： 123410:001,12:002,11:003,22:004;Smith:001,Jones:002,Johnson:003,Jones:004;Joe:001,Mary:002,Cathy:003,Bob:004;40000:001,50000:002,44000:003,55000:004; 这个布局，列更像是行式系统里的索引结构，这会让人们误以为：列式存储“不就是”每一列都带有索引的行式存储吗。其实这和数据映射还是有明显不同的。在面向行的索引系统里，主键是rowid，是从索引数据到主键的映射。而在面向列的系统里，主键是数据，是从数据到rowid的映射（译者注：文中的顺序与此相反）。这之间的区别很微妙，不过稍微修改一下存储方式就能看出来： 1…;Smith:001;Jones:002,004;Johnson:003;… 列式系统是否更高效很大程度上取决于工作负载的自动化（译者注：大概说的是一个操作从请求到响应之间的一系列工作workload）。获取一个给定对象的全部数据（一整行）就很慢。基于行的系统只要一次磁盘操作就可以获取到一行数据，而列式数据库里收集多列的数据就需要多次磁盘操作。不过，整行的操作比较罕见，大多数情况只需要数据的有限的子集。比如在名片系统，收集firstname和lastname来构建一个联系人列表就远比读取一个address的全部数据要常见。写入数据到数据库更是如此，尤其是数据很稀疏，很多列式可选的（optional）。因此，列存储尽管有很多理论上的缺陷，却依然展示出了优异的性能。 分区、索引、缓存、视图、OLAP cube和事务性系统（比如write-ahead logging或多版本并发控制）都会显著的影响任何一个系统的物理架构。也就是说，基于联机事务处理（OLTP）的RDBMS更多的面向行，而基于联机分析处理（OLAP）的系统则在面向行与列中权衡。 优势比较面向行与面向列数据库主要关注在给定工作负载下硬盘访问的效率。因为相比较计算机的其它瓶颈，寻址时间是相当长的。例如说，典型的SATA硬盘驱动器的平均寻址时间在16到22毫秒之间，而动态随机存取存储器（DRAM，也就是内存）在Intel Core i7处理器上的平均时间是60纳秒，几乎快400000倍之多。显然，处理大数据的主要瓶颈是磁盘访问。列式数据库通过减少磁盘访问的数量，和对相似列数据的有效压缩（译者注：同一列的数据是压缩存储）提升了系统的性能。 通过实践得知，列式系统非常适合类似联机分析处理（OLAP）的场景（比如数据仓库），这些系统通常会在全部数据上（PB级）执行高度复杂的查询。但是，总会有一些数据要写进列式数据库的，事务（INSERTs）必须分割成列并在存储时压缩，这使得它不适合OLTP场景。面向行的数据库非常适合于重度依赖交互事务的OLTP-like场景，比如说，当一行的全部数据位于一个单独的位置时，检索这些行数据会非常高效（因为最小化了磁盘寻址），正如面向行的体系架构。不过，面向列的系统已经发展成为可以处理OLTP和OLAP操作的混合系统，面向列的系统面临的一些OLTP的限制通过内存数据存储来解决。适合于OLAP和OLTP角色的基于列的系统，通过消除对单独系统的依赖（译者注：数据存储系统？），有效地减少了总数据占用。 压缩一列数据的格式时统一的，这意味着可以在存储大小上做一些列式数据能用而行式数据不能用的优化，例如很多流行的压缩方案，比如LWZ或游程编码，利用临近数据的相似性进行压缩。实际数据中常见的缺失值和重复值可以用两位标记来表示。相同的技术可以运用到行式数据上，但是效果差强人意。 为了提升压缩效率，对行进行排序是一个行之有效的方案。例如，使用位图索引排序可以将压缩性能提升一个量级。为了使字典顺序在运行长度编码方面的压缩效益最大化，最好使用低基数列作为第一个排序键（译者注：一脸懵逼）。例如，一个表含有性别、年龄和名称列，最好先对性别（基数为2）进行排序，然后对年龄进行排序（基数&lt;150），然后是名称（译者注：哦～）。 列压缩以牺牲检索效率为代价减少了磁盘空间，相邻压缩越大，随机访问就越困难，因为数据可能需要解压缩才能读取。因此，面向列的体系结构有时需要通过额外的机制来强化，旨在最大限度地减少访问压缩数据的需求（译者注：比如ORCFile格式的轻量级索引）。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一个博客]]></title>
    <url>%2F2019%2F09%2F28%2Fhello-blog%2F</url>
    <content type="text"><![CDATA[人生目标努力工作，给媳妇买！买！买！]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
